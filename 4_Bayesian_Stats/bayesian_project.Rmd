---
title: "Bayesian Modeling and Prediction for Movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    keep_md: yes
    theme: spacelab
---

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(BAS)
library(GGally)
library(knitr)
```

### Load data

```{r load-data}
if (!file.exists("movies.Rdata")) {
    url <- "https://d3c33hcgiwev3.cloudfront.net/_e1fe0c85abec6f73c72d73926884eaca_movies.Rdata?Expires=1504224000&Signature=LaWgAXetq-RnlqSzSP4NXpFPMb2WMHzEZPP91F8B85RC-7DCOFpscfYhIBXqEqYPQXh06s69LtnXhKR0XcL7WuhLZlNJ33lPtma7096827SwzowWliRGg6dQ33DTC4~KuilcHGJWq~GG7chFj81-fElIED-MQdcDgvsryqAE2MI_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A"
    download.file(url, dest = "movies.Rdata", mode = "wb") 
}
load("movies.Rdata")
```

* * *

## Part 1: Data

This project starts with the sample dataset as the previous project on linear regression and modelling. 

This given data set is comprised of 651 randomly sampled movies produced and released before 2016. The data was obtained from Rotten Tomatoes and IMDB APIs. We do not have any further details on how the random sampling was done, and so we will assume that it is a truly random sample.

We can understand the population this sample represents to be contemporary Hollywood movies. We do not have any data on the films' country of origin, but they appear to be mostly, if not exclusively, Hollywood productions.

Because this is an observational study with random sampling rather than an experiment with random assignment, we are not able to infer causality. Random sampling does however allow us to attempt to generalize findings from the sample to the larger population, in this case Hollywood movies.

We do not have any details on how the random sampling was done, and so it is difficult to assess potential biases in the sample. It would also be worth thinking closely about biases in certain variables in the data itself, particularly audience scores in comparison to critic's scores, how they may inherently differ and why.

* * *

## Part 2: Data Manipulation

We have been given a more specific task in this assignment: Develop a Bayesian regression model to predict `audience_score` from the explanatory variables below, five of which needed to be mutated.

```{r}
# create new variables
oscar_months <- c(10:12)
summer_months <- c(5:8)

# mutate new variables
movies <- movies %>%
    mutate(feature_film = factor(ifelse(title_type == "Feature Film", 
                                 "yes", "no")),
           drama = factor(ifelse(genre == "Drama", 
                                 "yes", "no")),
           mpaa_rating_R = factor(ifelse(mpaa_rating == "R", 
                                 "yes", "no")),
           oscar_season = factor(ifelse(
               thtr_rel_month %in% oscar_months, "yes", "no")),
           summer_season = factor(ifelse(
               thtr_rel_month %in% summer_months, "yes", "no")))

# remove unwanted variables
drops <- c("title","title_type","genre","mpaa_rating","studio",
           "thtr_rel_month","thtr_rel_day", "dvd_rel_year", 
           "dvd_rel_month","dvd_rel_day", "critics_rating",
           "audience_rating","director", "actor1", "actor2", "actor3", 
           "actor4","actor5", "imdb_url", "rt_url","dvd_rel")
films <- movies[ , !(names(movies) %in% drops)]

# remove one incomplete case
films <- films[complete.cases(films),]
```

Our dataset now has the 16 explanatory variables and 1 predictor variable `audience_score`.
```{r}
names(films)
```

* * *

## Part 3: Exploratory Data Analysis

We are now asked to perform EDA specifically for the newly created variables and their relationship with `audience_score`. In the previous assignment, EDA was done comparing `audience_score` and the given variables.

### feature_film

Most of the films in our dataset are in fact feature films. Non-feature films do have higher audience scores, but I think the important point here is to remove data points that may be unlike. 

```{r}
# Box+Dotplot of Audience Score by Feature Film
ggplot(films, aes(x = feature_film, y = audience_score)) + 
    geom_boxplot() +
    geom_dotplot(binaxis = 'y',
                 stackdir = 'center',
                 dotsize = 0.5,
                 fill = 'red',
                 binwidth = 3) +
    labs(x = "Feature Films", y = "Audience Score", 
         title = "Box+Dotplot of Audience Score by Feature Films")
```

We can see the exact summary statistics in the table below.

```{r}
# audience score by feature film summary stats
films %>%
    group_by(feature_film) %>%
    summarize(n = n(),
              min = min(audience_score),
              q1 = quantile(audience_score, 0.25),
              median = median(audience_score),
              mean = mean(audience_score),
              q3 = quantile(audience_score, 0.75),
              max = max(audience_score),
              sd = sd(audience_score))
```

### drama

Our dataset is fairly evenly split between dramas and non-dramas. Dramas do have slightly higher audience scores on average. Like `feature_film`, this variable will help us further define our dataset into like objects.

```{r}
# Box+Dotplot of Audience Score by Drama
ggplot(films, aes(x = drama, y = audience_score)) + 
    geom_boxplot() +
    geom_dotplot(binaxis = 'y',
                 stackdir = 'center',
                 dotsize = 0.5,
                 fill = 'red',
                 binwidth = 3) +
    labs(x = "Drama", y = "Audience Score", 
         title = "Box+Dotplot of Audience Score by Drama")
```

We can see the exact summary statistics in the table below.

```{r}
# audience score by feature film summary stats
films %>%
    group_by(drama) %>%
    summarize(n = n(),
              min = min(audience_score),
              q1 = quantile(audience_score, 0.25),
              median = median(audience_score),
              mean = mean(audience_score),
              q3 = quantile(audience_score, 0.75),
              max = max(audience_score),
              sd = sd(audience_score))
```

### mpaa_rating_R

`mpaa_rating_R` serves a similar purpose. Our dataset is fairly evenly divided between 'R' and 'non-R' rated films. The distributions are very similar.

```{r}
# Box+Dotplot of Audience Score by mpaa_rating_R
ggplot(films, aes(x = mpaa_rating_R, y = audience_score)) + 
    geom_boxplot() +
    geom_dotplot(binaxis = 'y',
                 stackdir = 'center',
                 dotsize = 0.5,
                 fill = 'red',
                 binwidth = 3) +
    labs(x = "MPAA Rating R", y = "Audience Score", 
         title = "Box+Dotplot of Audience Score by MPAA Rating R")
```

We can see the exact summary statistics in the table below.

```{r}
# audience score by mpaa_rating_R summary stats
films %>%
    group_by(mpaa_rating_R) %>%
    summarize(n = n(),
              min = min(audience_score),
              q1 = quantile(audience_score, 0.25),
              median = median(audience_score),
              mean = mean(audience_score),
              q3 = quantile(audience_score, 0.75),
              max = max(audience_score),
              sd = sd(audience_score))
```

### oscar_season

As we would expect, most films in our dataset are not released in Oscar season (defined as October, November or December theater release). The distribution of audience scores for Oscar and non-Oscar season films is fairly similar.

```{r}
# Box+Dotplot of Audience Score by Oscar Season
ggplot(films, aes(x = oscar_season, y = audience_score)) + 
    geom_boxplot() +
    geom_dotplot(binaxis = 'y',
                 stackdir = 'center',
                 dotsize = 0.5,
                 fill = 'red',
                 binwidth = 3) +
    labs(x = "Oscar Season", y = "Audience Score", 
         title = "Box+Dotplot of Audience Score by Oscar Season")
```

We can see the exact summary statistics in the table below.

```{r}
# audience score by oscar_season summary stats
films %>%
    group_by(oscar_season) %>%
    summarize(n = n(),
              min = min(audience_score),
              q1 = quantile(audience_score, 0.25),
              median = median(audience_score),
              mean = mean(audience_score),
              q3 = quantile(audience_score, 0.75),
              max = max(audience_score),
              sd = sd(audience_score))
```

### summer_season

Comparing `summer_season`, defined as a May through August theater release, to `audience_score` yields a similar result to `oscar_season`. It does not appear to have a profound impact on `audience_score`.

```{r}
# Box+Dotplot of Audience Score by Summer Season
ggplot(films, aes(x = summer_season, y = audience_score)) + 
    geom_boxplot() +
    geom_dotplot(binaxis = 'y',
                 stackdir = 'center',
                 dotsize = 0.5,
                 fill = 'red',
                 binwidth = 3) +
    labs(x = "Summer Season", y = "Audience Score", 
         title = "Box+Dotplot of Audience Score by Summer Season")
```

We can see the exact summary statistics in the table below.

```{r}
# audience score by summer_season summary stats
films %>%
    group_by(summer_season) %>%
    summarize(n = n(),
              min = min(audience_score),
              q1 = quantile(audience_score, 0.25),
              median = median(audience_score),
              mean = mean(audience_score),
              q3 = quantile(audience_score, 0.75),
              max = max(audience_score),
              sd = sd(audience_score))
```

### Multivariable EDA

We can try plotting many of these variables at once. But honestly, I think viewing the above plots makes it easier to get a sense of where the data lies.

```{r}
films$season <- ifelse(films$oscar_season == 'no' &
                       films$summer_season == 'no', 'None',
                       ifelse(films$summer_season == 'yes', 'Summer', 'Oscar'))
films$season <- as.factor(films$season)

# multivariable plot
ggplot(films, aes(x = runtime, y = audience_score, 
                  color = drama, 
                  shape = feature_film)) +
    geom_point(alpha = 0.8) +
    facet_wrap(~ season)
```

I created a `season` variable to make this plot, but will now remove it as it is not called for in the requirements. Although it should not affect any calculations as it represents no new information, it is unnecessary.
```{r}
# drop season column
films <- select(films, -season)
names(films)
```

* * *

## Part 4: Modeling

### Model selection

#### Simple Linear Regression Model

Before building the requested multivariable Bayesian linear regression model, I'll start with a simple linear regression model, but in a Bayesian framework. I'll use `imdb_rating` to predict `audience_score` since they are the most correlated variables in the dataset.

I will use a reference prior distribution that provides a connection between the frequentist solution and Bayesian answers. This provides a baseline analysis for comparison with more informative prior distributions. Under the reference prior, the posterior mean and standard deviation is equal to the OLS estimate and standard error respectively.

```{r}
# simple linear regression model, reference prior
slr <- lm(audience_score ~ imdb_rating, data = films)
summary(slr)
```
```{r}
# slr added to plot
ggplot(films, aes(x = imdb_rating, y = audience_score)) + 
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE)
```

Under a reference prior, point estimates and Bayesian credible intervals are equivalent to Frequentist estimates and confidence intervals. The key difference is in the Bayesian interpretation of credible intervals. We can see these figures calculated in the table below.

```{r}
out = summary(slr)$coef[, 1:2]
out = cbind(out, confint(slr))
colnames(out) = c("posterior mean", "sd", "2.5", "97.5")
round(out, 2)
```

I'll save coefficient interpretation for the models below.

#### Bayesian Model Selection: Stepwise

We can now try to fit Bayesian multiple regression models using the techniques provided for in the `BAS` package.

Using the `BAS` package, I'll use a stepwise selection method based on minimizing BIC (Bayesian Information Criteria). 

```{r}
n = nrow(films)
films.lm = lm(audience_score ~ ., data = films)
films.step = step(films.lm, k = log(n)) # penalty for BIC rather than AIC
```

From the intial full model that had sixteen explanatory variables, we've gone down to three variables, reducing BIC (labeled AIC) from 3083.05 to 3020.53.

We can calculate BIC directly with the `BIC` function. Comparing the full model to the best model shows a reduction, although I'm not sure why the values are quite different.
```{r}
best <- lm(audience_score ~ runtime + critics_score + imdb_rating, data = films)
BIC(best)
full <- lm(audience_score ~ . - audience_score, data = films)
BIC(full)
```

#### Bayesian Model Selection: The Best Model

We can also use the `BAS` package to enumerate all 2^16 possible models and determine what single model yields the lowest BIC. Setting `modelprior` to uniform assigns equal probabilities to all possible models.

```{r}
# use the BAS package to fit all possible BIC models
films.bic = bas.lm(audience_score ~ ., data = films, 
                 prior = "BIC", 
                 modelprior = uniform())

# find the best BIC model with BAS
best = which.max(films.bic$logmarg)  # this is the best BIC model

# indices of the best model where 0 is the intercept
bestmodel =  films.bic$which[[best]]

bestgamma = rep(0, films.bic$n.vars)
# convert to a binary  vector where 1 means the variable was selected
# use this to fit the best BIC model
bestgamma[bestmodel + 1] = 1  

films.bestbic = bas.lm(audience_score ~ ., data = films,
                     prior = "BIC", n.models = 1,
                     bestmodel = bestgamma, 
                     modelprior = uniform())
```

Having selected the best model according to BIC (the same model we reached in the stepwise process above), we can then extract posterior means and standard deviations using the `coef` function.

```{r}
films.coef = coef(films.bestbic)

out = confint(films.coef)
names = c("post mean", "post sd", colnames(out))
coef.bic = cbind(films.coef$postmean, films.coef$postsd, out)
colnames(coef.bic) = names
coef.bic
```

The posterior estimates and intervals for all coefficients except the intercept are the same using either `lm` or `BAS` and have the same Bayesian interpretation whether or not the predictor variables have been centered. We can see this below if we create the same model reached through the stepwise selection with the `lm` function.

```{r}
# create the same model with lm
best.bic =  lm(audience_score ~ runtime + imdb_rating + critics_score, data = films)
out = summary(best.bic)$coef[, 1:2]
colnames(out) = c("posterior mean", "sd")
out = cbind(out, confint(best.bic))
out
```

#### Bayesian Model Averaging

While we have determined a single best model according to BIC, there is often uncertainty around which is definitively the best model. We can take an ensemble approach and use it to compare models.

Instead of mandating the `bas.lm` function to find only the best model, it can look at all possible models. The `summary` function will report results from the top five models.

```{r}
# summary of top five models
films_bas = bas.lm(audience_score ~ ., 
                 prior = "BIC", 
                 modelprior = uniform(),
                 data = films)
summary(films_bas)
```

Here we learn the posterior probabilities, Bayes Factors, and R^2 among other statistics for the top five models. We can see that model 1 has the three explanatory variables we saw included in the 'best' model above. An advantage of not looking at just the best model though is that we can see, in this case, that model 2 is very similar. Although it excludes `runtime`, its posterior probability and R^2 are nearly the same.

The posterior probability of model 1 is only 12.97%. This is much higher than the probability of any given model under a uniform prior (1/2^16).

The print method returns the call and the marginal posterior inclusions probabilites of the variables. For example, `runtime` is included in about 47% of models.

```{r}
print(films_bas)
```

The `image` function helps us visualize the model space under this ensemble approach.

```{r}
image(films_bas, rotate = F)
```

Here we can see the inclusion of predictor variables on the left hand side in various models ranked from left to right, with area reflecting the log posterior odds. The color black represents the variable is not included in the model, while a color means it is. 

We can see for example that `imdb_rating` is included in all of the most likely models, while `drama` was not included in any of the most likely models. The red coloring shared by models 1 and 2 reflect that they have very similar posterior probabilities-- this was confirmed in the above table. Models 3, 4, 5, 6 -- colored in pink -- all had posterior probabilities around 3%. The differences between color groups suggest that the differences between models are "not worth a bare mention".

To calculate the posterior distributions of the coefficients, we use the `coef` function create an object with posterior means, standard deviations, and posterior inclusion probabilities obtained by BMA. This table below is the analog to the frequentist summary table, with the posterior inclusion probability replacing the role of the p-value.

Importantly, the values calculated below take into account not just one model, but all possible models generated from having 16 predictor variables (2^16).

```{r}
films_coef = coef(films_bas)
films_coef
```

We can also extract credible intervals using the `confint` function.

```{r}
confint(films_coef, nsim = 10000)
```

Lastly, we can plot the posterior distributions under BMA of each regression coefficient. The height of the vertical bar represents the posterior probability that the coefficient is 0. The bell-shaped curve represents the density of plausible values from all the models where the coefficient was non-zero. This is scaled so that the height of the density for non-zero values is the probability that the coefficient is non-zero. 

For example, `imdb_rating` has no bar since it is essentially certain that it should be included. Most other variables have spikes close to 1. In these cases where the variable is forced into the model, the distribution overlaps 0.

```{r}
par(mar = c(1,1,1,1))
par(mfrow = c(8,2))
plot(films_coef, subset = c(2:16), ask = F)
title("Posterior Distributions of Regression Coefficients under BMA",
      outer = TRUE)
```

#### Bayesian Model Coefficient and Interval Interpretation

Here I'll interpret the coefficients we just calculated through BMA.

The mean estimate interpretation, as I understand it, is largely the same. For example, holding all else constant, a one unit increase in `imdb_rating` is associated with, on average, a 14.98 point increase in `audience_score`. 

There is an important difference however in the interpretation of the credible interval in the Bayesian paradigm vs. the confidence interval in the Frequentist paradigm.

The frequentist definition of probability allows us to define a probability for the confidence interval procedure but not for specific fixed sample. (For example, 95% of similarly constructed intervals will contain the true mean).

The Bayesian definition is a bit more flexible-- and direct. Since it is a measure of belief it allows us to describe the unknown true parameter not as a fixed value but with a probability distribution. We can say that the probability that the true mean of a sample is within a given interval is 0.95.

And so in the case of `imdb_rating`, we can say that the probability that the slope coefficient lies between 13.66 and 16.52 is 95%.

Our second most included variable was `critics_score`. Holding all else constant, a one unit increase in `critics_score` is associated with, on average, a 0.063 point increase in `audience_score`. And we can say that the probability that this slope coefficient lies between 0 and 0.11 is 95%. 

This would strongly suggest to us that `critics_score` might play some role in predicting `audience_score` but ultimately a broad popularity rating is much more closely associated.

Most other predictor variables have a negative lower bound and a positive upper bound, close to 0, suggesting these variables have very little impact.

#### Priors for Bayesian Model Uncertainty

Above anaylsis used BIC to determine Bayes factors, but there are other prior distributions we can use running `BAS`.

```{r}
n <- nrow(films)

films.g = bas.lm(audience_score ~ ., data = films, prior = "g-prior", 
               a = n, modelprior = uniform())
# a is the hyperparameter in this case g=a
films.ZS = bas.lm(audience_score ~ ., data = films, prior = "ZS-null", 
                a = n, modelprior = uniform())
films.BIC = bas.lm(audience_score ~ ., data = films, prior = "BIC", 
                 a = n, modelprior = uniform())
films.AIC = bas.lm(audience_score ~ ., data = films, prior = "AIC", 
                 modelprior = uniform())
films.HG = bas.lm(audience_score ~ ., data = films, prior =
                      "hyper-g-n", a = 3, modelprior = uniform()) 
# hyperparameter a=3
films.EB = bas.lm(audience_score ~ ., data = films, prior = "EB-local",
                a = n, modelprior = uniform())

```

We can plot the marginal inclusion probabilities for each method (BIC, g, Zellner-Siow-Cauchy, hyper g, Empirical Bayes, and AIC). All methods agree we should include `imdb_rating`. `critics_score` has a probability of inclusion greater than 0.8 for all methods. Most methods place `runtime` slightly more than 0.4. Most methods look quite similar with AIC being less conservative, which is not surprusing because AIC is designed to find good predictive models.

```{r}
probne0 = cbind(films.BIC$probne0, films.g$probne0, films.ZS$probne0,  
                films.HG$probne0, films.EB$probne0, films.AIC$probne0)
colnames(probne0) = c("BIC","g", "ZS", "HG", "EB", "AIC")
rownames(probne0) = c(films.BIC$namesx)

myblue = rgb(86,155,189, name = "myblue", max = 256)
mydarkgrey = rgb(.5,.5,.5, name = "mydarkgrey", max = 1)

par(mar = c(1,1,1,1))
par(mfrow = c(6,3))
for (i in 2:16) {
  barplot(height = probne0[i,], ylim = c(0,1), main =
              films.g$namesx[i], col.main = mydarkgrey, col = myblue)
}
title("Marginal Inclusion Probabilities", outer = TRUE)
```

#### Bayesian Model Diagnostics

We should always check model diagnostics.

##### Residuals versus fitted values using BMA

As with `lm` we would like to see a uniform spread of points for each fitted value indicating that the constant variance assumption is acceptable. This doesn't seem to entirely be the case for lower rated films. We saw this in our basic scatterplot.

```{r}
# residual plot
plot(films_bas, which = 1,add.smooth = F, ask = F, pch = 16,
     sub.caption = "", caption = "")
```

##### Cumulative sampled probability

This is a plot of the cumulative probability of the unique models that were sampled where every time a new model is discovered there is a jump in the cumulative probability. Ideally this levels off suggesting that each additional model has a very small probabilty and does not contribute substantially to the posterior distribution.

This does not appear to be the case here but our cumulative probability definitely tailed off even after five models according to the tables above.

```{r}
# cumulative sampled probability
plot(films_bas, which = 2, add.smooth = F)
```

##### Model complexity

Next is the plot of model size vs the log of the marginal likelihood to compare each model to null model. We get a plot with three horizontal bands of points. The band with the highest marginal likelihood, I believe, would likely represent models with `imdb_rating` included. At the top left you see the model with the highest log marginal likelihood only has 2 predictors. Then it slightly trails off as more variables are added.

```{r}
# model complexity
plot(films_bas, which = 3, ask = F, caption = "", sub.caption = "")
```

##### Marginal Inclusion probabilities

The lines in blue correspond to the variables where the marginal posterior inclusion probability, or PIP, is greater than 0.5 suggesting that these variables are important for predicting `audience_score`. `runtime` is just below the 0.5 mark.

```{r}
# Marginal Inclusion probabilities
plot(films_bas, which = 4, ask = F, caption = "", sub.caption = "", 
     col.in = myblue, col.ex = mydarkgrey, lwd = 3)
```

#### Frequentist Paradigm Comparison

Although not required here I wanted to quickly check how the best model under the Bayesian paradigm compared to the model we would arrive at in the frequentist paradigm. I've adopted the backwards elimination method, taking p-value as the decision criteria.

```{r}
# start with the full model
mod_full <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
              best_pic_win + best_actor_win + best_actress_win +
              best_dir_win + top200_box + feature_film + drama +
              mpaa_rating_R + oscar_season + summer_season, data =
              films)
summary(mod_full)
```

Now remove variables one at a time, starting with the highest p-value, until no variable has a p-value greater than 0.05.

```{r}
# least significant variable
tail(cbind(sort(summary(mod_full)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod1 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
              best_pic_win + best_actor_win + best_actress_win +
              best_dir_win + feature_film + drama +
              mpaa_rating_R + oscar_season + summer_season, data =
              films)
# least significant variable
tail(cbind(sort(summary(mod1)$coefficients[,4])),1)
```

```{r}
# drop least significant variable
mod2 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
              best_pic_win + best_actor_win + best_actress_win +
              best_dir_win + feature_film + drama +
              mpaa_rating_R + summer_season, data =
              films)
# least significant variable
tail(cbind(sort(summary(mod2)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod3 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
               best_actor_win + best_actress_win +
              best_dir_win + feature_film + drama +
              mpaa_rating_R + summer_season, data =
              films)
# least significant variable
tail(cbind(sort(summary(mod3)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod4 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
               best_actor_win + best_actress_win + feature_film + drama
           + mpaa_rating_R + summer_season, data = films)
# least significant variable
tail(cbind(sort(summary(mod4)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod5 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
               best_actor_win + best_actress_win + feature_film + drama
           + mpaa_rating_R + summer_season, data = films)
# least significant variable
tail(cbind(sort(summary(mod5)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod6 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
               best_actor_win + best_actress_win + feature_film + drama
           + mpaa_rating_R, data = films)
# least significant variable
tail(cbind(sort(summary(mod6)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod7 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
               best_actor_win + best_actress_win + feature_film + drama
           + mpaa_rating_R, data = films)
# least significant variable
tail(cbind(sort(summary(mod7)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod8 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
               best_actor_win + best_actress_win + drama
           + mpaa_rating_R, data = films)
# least significant variable
tail(cbind(sort(summary(mod8)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod9 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
              imdb_num_votes + critics_score + best_pic_nom +
               best_actor_win + best_actress_win + mpaa_rating_R, data 
           = films)
# least significant variable
tail(cbind(sort(summary(mod9)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod10 <- lm(audience_score ~ runtime + thtr_rel_year + imdb_rating +
                critics_score + best_pic_nom + best_actor_win +
                best_actress_win + mpaa_rating_R, data = films)
# least significant variable
tail(cbind(sort(summary(mod10)$coefficients[,4])),2)
```
```{r}
# drop least significant variable
mod11 <- lm(audience_score ~ runtime + imdb_rating +
                critics_score + best_pic_nom + best_actor_win +
                best_actress_win + mpaa_rating_R, data = films)
# least significant variable
tail(cbind(sort(summary(mod11)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod12 <- lm(audience_score ~ runtime + imdb_rating +
                critics_score + best_pic_nom +
                best_actress_win + mpaa_rating_R, data = films)
# least significant variable
tail(cbind(sort(summary(mod12)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod13 <- lm(audience_score ~ runtime + imdb_rating +
                critics_score + best_pic_nom + mpaa_rating_R, data =
                films)
# least significant variable
tail(cbind(sort(summary(mod13)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod14 <- lm(audience_score ~ runtime + imdb_rating +
                critics_score + mpaa_rating_R, data =
                films)
# least significant variable
tail(cbind(sort(summary(mod14)$coefficients[,4])),1)
```
```{r}
# drop least significant variable
mod15 <- lm(audience_score ~ runtime + imdb_rating +
                critics_score, data = films)
# all variables now significant p-values
cbind(sort(summary(mod15)$coefficients[,4]))
```

You'll notice this is the same model we arrived at by minimizing BIC.

After removing thirteen variables, our parsimonious model retains only the variables `runtime`, `imdb_rating`, and `critics_score`. Our R^2 has hardly changed. R^2 for the full model was `r round(summary(mod_full)$r.squared,3)`, compared to `r round(summary(mod15)$r.squared,3)` with only three explanatory variables.

```{r}
summary(mod15)
```

But of course this model gives us way less information than we got through the BMA approach. The Bayesian credible intervals also allow us to speak more directly about our results.

* * *

## Part 5: Prediction

### Decision Making Under Uncertainty

There is no single way to choose the best model. Rather it depends on the context of the situation. Selecting a model should be based on some decision process that takes into account the purpose of the analysis and cost and losses associated with selection.

Given that we are primarily interested in predicting `audience_score`, we might favor a model that generates predictions most similar to BMA and the corresponding posterior standard deviations. This is BPM.

Other choices includes the Median Probability Model (MPM), which choose a model in which all variables have posterior inclusion probabilities greater than 0.5, or Highest Probability Model (HPM), which chooses the model with the highest posterior probability.

#### HPM

```{r}
HPM = predict(films_bas, estimator = "HPM",  se.fit = TRUE)
(films_bas$namesx[HPM$bestmodel + 1])[-1]
```

#### MPM

```{r}
MPM = predict(films_bas, estimator = "MPM", se.fit = TRUE)
films_bas$namesx[MPM$bestmodel + 1]
```

#### BPM

```{r}
BPM = predict(films_bas, estimator = "BPM", se.fit = TRUE)
films_bas$namesx[BPM$bestmodel + 1]
```

#### Comparison

```{r}
BMA = predict(films_bas, estimator = "BMA",  se.fit = TRUE)

par(cex = 1.8, cex.axis = 1.8, cex.lab = 2, mfrow = c(2,2), mar = c(5,5, 3, 3), col.lab = mydarkgrey, col.axis = mydarkgrey, col = mydarkgrey)

ggpairs(data.frame(HPM = as.vector(HPM$fit),  #this used predict so we need to extract fitted values
                   MPM = as.vector(MPM$fit),  # this used fitted
                   BPM = as.vector(BPM$fit),  # this used fitted
                   BMA = as.vector(BMA$fit))) # this used predict
```

We see that these are highly correlated with BMA. In fact, they all round to 0.999.

Letâ€™s turn to see what characteristics lead to the highest predicted `audience_score` with the BPM model.

```{r}
opt = which.max(BPM$fit)
t(films[opt, ])
```

The results here are not too surprising. To get the highest predicted `audience_score` it inputs a theoretical R-rated feature film, released in Oscar season of 1974, with very high `imdb_rating` and `critics_score` and lots of Oscar wins. For some reason though, this theoretical film would not be in the box office top 200. 

We can onbtain a 95% credible interval for the predicted `audience_score` of a film with the covariates specified above.

```{r}
ci_audience_score = confint(BPM, parm = "pred")
ci_audience_score[opt,]
```

### Out of Sample Predictions

Now let's test the model on some real movies out of our sample. We would expect variability to increase moving away from the center so, for out of sample predictions, I wanted to test predictions for a film highly-rated, poorly-rated, and mediocre: 12 Years a Slave, The Lone Ranger, and The Wolverine. The data for these films comes from IMDB and Rotten Tomatoes.

```{r}
# dataframes for three new movies

# 12 Years a Slave: audience_score = 91
twelve_years <- data.frame(runtime = 134, 
                                  thtr_rel_year = 2013,
                                  imdb_rating = 8.1,
                                  imdb_num_votes = 507018,
                                  critics_score = 96,
                                  best_pic_nom = 'yes',
                                  best_pic_win = 'yes',
                                  best_actor_win = 'no',
                                  best_actress_win = 'no',
                                  best_dir_win = 'no',
                                  top200_box = 'yes',    
                                  feature_film = 'yes',
                                  drama = 'yes',
                                  mpaa_rating_R = 'yes', 
                                  oscar_season = 'yes', 
                                  summer_season = 'no')

# The Lone Ranger: audience_score = 51
lone_ranger <-  data.frame(runtime = 150, 
                                  thtr_rel_year = 2013,
                                  imdb_rating = 6.5,
                                  imdb_num_votes = 195530,
                                  critics_score = 31,
                                  best_pic_nom = 'no',
                                  best_pic_win = 'no',
                                  best_actor_win = 'no',
                                  best_actress_win = 'no',
                                  best_dir_win = 'no',
                                  top200_box = 'yes',    
                                  feature_film = 'yes',
                                  drama = 'no',
                                  mpaa_rating_R = 'no', 
                                  oscar_season = 'no', 
                                  summer_season = 'yes')

# The Wolverine: audience_score = 69
wolverine <-  data.frame(runtime = 126, 
                                  thtr_rel_year = 2013,
                                  imdb_rating = 6.7,
                                  imdb_num_votes = 366887,
                                  critics_score = 69,
                                  best_pic_nom = 'no',
                                  best_pic_win = 'no',
                                  best_actor_win = 'no',
                                  best_actress_win = 'no',
                                  best_dir_win = 'no',
                                  top200_box = 'yes',    
                                  feature_film = 'yes',
                                  drama = 'no',
                                  mpaa_rating_R = 'no', 
                                  oscar_season = 'no', 
                                  summer_season = 'yes')
```

We can supply these dataframes as new data to the `predict` function.

```{r}
BPM.twelve_years = predict(films_bas, newdata = twelve_years, estimator
                           = "BPM", se.fit = TRUE)
BPM.twelve_years.conf.fit = confint(BPM.twelve_years, parm = "mean")
a <- BPM.twelve_years.conf.fit[,1:3]

BPM.lone_ranger = predict(films_bas, newdata = lone_ranger,
                          estimator = "BPM", se.fit = TRUE)
BPM.lone_ranger.conf.fit = confint(BPM.lone_ranger, parm = "mean")
b <- BPM.lone_ranger.conf.fit[,1:3]

BPM.wolverine = predict(films_bas, newdata = wolverine,
                        estimator = "BPM", se.fit = TRUE)
BPM.wolverine.conf.fit = confint(BPM.wolverine, parm = "mean")
c <- BPM.wolverine.conf.fit[,1:3]

# format output
mat <- matrix(c(a,b,c), ncol = 3, byrow = TRUE, dimnames = list(c("12 Years a Slave", "The Lone Ranger", "The Wolverine"), c("2.5%","97.5%", "Mean")))

pred_df <- mat %>%
    as.data.frame() %>%
    mutate(Actual = c(95,67,68),
           Film = c("12 Years a Slave", "The Lone Ranger", "The Wolverine"))
pred_df <- pred_df[,c(5,1,2,3,4)]
kable(pred_df)
```

Our model estimates are quite close for 12 Years a Slave and The Wolverine. It was not so good for The Lone Ranger, but we might expect this would have been more difficult to predict given a greater disagreement between its `imdb_rating` and `audience_score`. 

While the mean estimates are close, it looks like our credible intervals may be too small. 12 Years a Slave and The Lone Ranger exceed the upper bound of their 95% credible interval. Wolverine nearly does so.

### Frequentist Predictions Comparison

I wanted to compare these predictions to the estimates and confidence intervals produced from the parsimonious linear model we arrived at through backwards elimination p-value method.

```{r}
# 12 Years a Slave: audience_score = 91
predict(mod15, newdata = data.frame(runtime = 134, imdb_rating = 8.1, 
                                    critics_score = 96), interval =
                                        "predict")
# The Lone Ranger: audience_score = 51
predict(mod15, newdata = data.frame(runtime = 150, imdb_rating = 6.5, 
                                    critics_score = 31), interval =
                                        "predict")
# The Wolverine: audience_score = 69
predict(mod15, newdata = data.frame(runtime = 126, imdb_rating = 6.7, 
                                    critics_score = 69), interval =
                                        "predict")
```

If you compare these statistics with the predicted values from our Bayesian model, you will notice that the mean estimates themselves are quite similar to our Bayesian predictions. However, our confidence intervals are much wider than the Bayesian credible intervals. Perhaps this is necessary given that in at least two cases the actual values were outside the 95% credible interval.

### Training and Testing Data

Lastly I wanted to copy the technique of the Week 4 Supplement Lab.

It documents an approach to help select among alternate estimators by using out of sample validation to compare predictive accuracy. The following code creates a training data and test data set based on a left out 10% sample. Next we obtain the posterior distribution of models and model specific parameters using `BAS` using only the training data. 

Using this posterior distribution we then predict `audience_score` using the explanatory variables in the test data set for several estimators: BMA, BPM, MPM and HPM and extract the predicted (fit) values. 

The function `cv.summary.bas` computes the average square root mean squared error between the fitted values and the observed values of `audience_score` in the left out test data.

```{r}
set.seed(42)

n = nrow(films)
n_cv = 50
ape = matrix(NA, ncol=4, nrow=n_cv)
colnames(ape) = c("BMA", "BPM", "HPM", "MPM")

for (i in 1:n_cv) {
  train = sample(1:n, size=round(.90*n), replace=FALSE)
  films_train = films[train,]
  films_test = films[-train,]

  bma_train_films = bas.lm(audience_score ~ . - audience_score,
                           data = films_train, 
                          prior = "BIC", modelprior = uniform(),
                          initprobs = "eplogp")
  yhat_bma = predict(bma_train_films, films_test, estimator="BMA")$fit
  yhat_hpm = predict(bma_train_films, films_test, estimator="HPM")$fit
  yhat_mpm = predict(bma_train_films, films_test, estimator="MPM")$fit
  yhat_bpm = predict(bma_train_films, films_test, estimator="BPM")$fit
  ape[i, "BMA"] = cv.summary.bas(yhat_bma, films_test$audience_score)
  ape[i, "BPM"] = cv.summary.bas(yhat_bpm, films_test$audience_score)
  ape[i, "HPM"] = cv.summary.bas(yhat_hpm, films_test$audience_score)
  ape[i, "MPM"] = cv.summary.bas(yhat_mpm, films_test$audience_score)
}
```

I'm not sure how to interpret the warning, but we can construct side-by-side boxplots of the average prediction error and the mean of the APE over the different test sets.

```{r}
boxplot(ape)
```

```{r}
apply(ape, 2, mean)
```

While `BMA` is the best, followed by `BPM`, they are all really close in this case. 

* * *

## Part 6: Conclusion

We set out to examine what factors influence `audience_score`, and in particular to try to predict it form a host of possible explanatory variables.

### Key Findings

Despite all the work we did, `imdb_rating` ultimately does the heavy lifting in the model. We saw from the first scatterplot that these variables were highly correlated and not surprisingly it has far more predictive capability than any other variable.

An important question the studio may have though is the relative importance of `critics_score`. It certainly looks like from this analysis, while it may play some role, it is dwarfed in comparison to `imdb_rating`. This may suggest to them that for advance screenings or testing samples, regular moviegoers may be better predictors than critics.

### Shortcomings

1. Although we should ideally test predicting more films, it looks like our credible intervals were generally too small. While these films could be considered outliers in a sense (an Oscar winner and a widely-panned film), there is nothing inherently wrong with them. That being said, our estimates were fairly close.

2. The Bayesian approach allows us to incorporate our prior beliefs. Even though we have some initial beliefs about what variables will be important for predicting `audience_score`, we used a prior distribution that assumes all models are equally likely. This corresponds to a prior probability of 0.5 that a coefficient is non-zero. We could set different variables with unique prior probabilities of being included based on expert opinion of what we already know. For example, we could have set `imdb_rating` to a prior probability of 1 so it would always be included and reduce uncertainty. 

3. Lastly, given the amount of data available from Rotten Tomatoes and IMDB APIs, it seems like there is no reason we couldn't easily collect more data to see how that might impact our model and predictive capabilities.
